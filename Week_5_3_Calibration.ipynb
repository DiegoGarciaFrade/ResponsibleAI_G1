{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Robustness at The Prediction Stage: Model Calibration\n",
    "\n",
    "Model calibration ensures that the model's predicted probabilities accurately reflect the true likelihood of the target event. Calibrated models yield more trustworthy and interpretable probabilities, aiding informed decision-making.\n",
    "   \n",
    "Here, we try to showcase how the calibration affects the prediction of a SVC classifier on the adapted Diabetes dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Packages and Data Preparation\n",
    "\n",
    "In this example, to better showcase the difference, we create an unbalanced dataset by using mean (instead of median) outputs as the threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes_data = datasets.load_diabetes()\n",
    "X = diabetes_data.data\n",
    "y = diabetes_data.target\n",
    "threshold = np.mean(y)\n",
    "y_binary = (y > threshold).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into training/Vlidation/Test Sets:\n",
    "It is important to perform the calibration on the validation set to prevent data leakage and at the same time avoid overfitting on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y_binary, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardiation and Model fitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Train uncalibrated SVC on the training set\n",
    "svc = SVC(probability=True)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Calibration on the Validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate the SVC model on the validation set\n",
    "svc_calibrated = CalibratedClassifierCV(estimator=svc, cv='prefit', method='isotonic')\n",
    "svc_calibrated.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the Claibration of Prediction before/after Model Calibration\n",
    "\n",
    "We can use calibration curves and brier score to compare the calibration of the predictions before/after the model calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict probabilities on the test set\n",
    "probs_svc = svc.predict_proba(X_test)[:, 1]\n",
    "probs_svc_calibrated = svc_calibrated.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Compute calibration curve for SVC\n",
    "fraction_of_positives_svc, mean_predicted_value_svc = calibration_curve(y_test, probs_svc, n_bins=10)\n",
    "\n",
    "# Compute calibration curve for calibrated SVC\n",
    "fraction_of_positives_svc_calibrated, mean_predicted_value_svc_calibrated = calibration_curve(y_test, probs_svc_calibrated, n_bins=10)\n",
    "\n",
    "# Compute Brier scores\n",
    "brier_uncalibrated = brier_score_loss(y_test, probs_svc)\n",
    "brier_calibrated = brier_score_loss(y_test, probs_svc_calibrated)\n",
    "\n",
    "print(\"Brier Score (Uncalibrated):\", brier_uncalibrated)\n",
    "print(\"Brier Score (Calibrated):\", brier_calibrated)\n",
    "\n",
    "# Plot the calibration curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.plot(mean_predicted_value_svc, fraction_of_positives_svc, marker='o', label='Uncalibrated SVC')\n",
    "plt.plot(mean_predicted_value_svc_calibrated, fraction_of_positives_svc_calibrated, marker='x', label='Calibrated SVC')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "plt.title('Calibration Curve - SVC')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

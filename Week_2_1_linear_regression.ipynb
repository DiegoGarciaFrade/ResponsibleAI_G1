{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Diabetes dataset from scikit-learn (version '1.0.2') to demonstrate the Coefficient Magnitude method for explianing a Linear Regression model.\n",
    "\n",
    "The \"Diabetes\" dataset contains ten baseline variables (such as age, BMI, blood pressure, etc.) for a set of diabetes patients and a target variable (a quantitative measure of disease progression one year after baseline). \n",
    "\n",
    "Here are feature description:\n",
    "\n",
    "* age: age in years\n",
    "* sex\n",
    "* bmi: body mass index\n",
    "* bp: average blood pressure\n",
    "* s1: tc, total serum cholesterol\n",
    "* s2: ldl, low-density lipoproteins\n",
    "* s3: hdl, high-density lipoproteins\n",
    "* s4: tch, total cholesterol / HDL\n",
    "* s5: ltg, possibly log of serum triglycerides level\n",
    "* s6: glu, blood sugar level\n",
    "\n",
    "The aim is to use these features to predict a quantitative measure of disease progression one year after baseline.\n",
    "\n",
    "We first load the \"Diabetes\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes_data = load_diabetes()\n",
    "X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y = diabetes_data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features is a best practice when using linear models, as it ensures that each feature contributes equally to the model's predictions and prevents any feature from dominating the others due to its scale.\n",
    "\n",
    "**Food for Thought**: How standardization does facilitate the explanation of linear models? Dose it make sense to standardize binary features (such as sex in this case)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing features\n",
    "scaler = StandardScaler()\n",
    "X_train_standardized = scaler.fit_transform(X_train)\n",
    "X_test_standardized = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Create and fit the Random Forest Regressor model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_standardized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_standardized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the performance of the regression model is crucial to understand how well it fits the data. One common way to assess the model's goodness of fit is by using an R-squared plot. The R-squared (coefficient of determination) measures the proportion of the variance in the dependent variable that is predictable from the independent variables. A higher R-squared value indicates a better fit of the model to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R-squared value\n",
    "r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the coefficients and feature names\n",
    "coefficients = model.coef_\n",
    "feature_names = diabetes_data.feature_names\n",
    "\n",
    "# Create a DataFrame to store the coefficients with their corresponding feature names\n",
    "coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': coefficients})\n",
    "\n",
    "# Sort the DataFrame by coefficient magnitude in descending order\n",
    "coeff_df = coeff_df.reindex(coeff_df['Coefficient'].abs().sort_values(ascending=False).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Actual vs Predicted scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted: Linear Regression')\n",
    "plt.text(50, 300, f'R-squared: {r2:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "\n",
    "# Create a bar plot to visualize coefficient magnitudes\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(coeff_df['Feature'], coeff_df['Coefficient'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Coefficient Magnitude')\n",
    "plt.title('Coefficient Magnitude in Linear Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we create an Actual vs Predicted scatter plot to visualize the model's performance. The scatter plot displays the actual target values on the x-axis and the corresponding predicted values on the y-axis. The diagonal dashed line represents perfect predictions, where the actual and predicted values would be identical.\n",
    "\n",
    "The bar plot will now display the feature importance based on the standardized coefficient magnitudes for the \"Diabetes\" dataset. The standardized coefficients allow us to directly compare the relative importance of features, even when they are on different scales.\n",
    "\n",
    "Again, please note that the Coefficient Magnitude method is specific to linear models like Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Coefficient Magnitudes to Feature Importance\n",
    "\n",
    "To move one step further in more robust interpretation of a linear model, we can transform the coefficient magnitude into feature importance by deviding the individual coefiicients by their standard deviation. To calculated the standard deviation of coefficients we need to use a bag of Linear Regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# Creating, estimating, and predicting using a bag of 100 Linear regressor models.\n",
    "n_estimators = 100\n",
    "\n",
    "# mind that in the newer sklearn versions (the latest) the first input is called 'estimator' instead of 'base_estimator'\n",
    "bagged_model = BaggingRegressor(estimator=LinearRegression(), \n",
    "                                n_estimators=n_estimators, \n",
    "                                random_state=random_state)\n",
    "bagged_model.fit(X_train_standardized, y_train)\n",
    "\n",
    "y_pred = bagged_model.predict(X_test_standardized)\n",
    "\n",
    "# Calculate R-squared value\n",
    "r2 = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can retrieve the weights of every single model in our bag and use the following formula to convert the coefficient magnitude $\\beta$ for the $j$ th feature into feature importances: $\\frac{\\bar{\\beta}_j}{SE(\\beta_j)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = []\n",
    "for i in range(n_estimators):\n",
    "    coefficients.append(bagged_model.estimators_[i].coef_)\n",
    "coefficients = np.vstack(coefficients)\n",
    "\n",
    "feature_importance = np.mean(coefficients, axis=0) / (np.std(coefficients, axis=0)/n_estimators**0.5)\n",
    "\n",
    "\n",
    "# Create a DataFrame to store the feature importance with their corresponding feature names\n",
    "fi_df = pd.DataFrame({'Feature': feature_names, 't_stats': feature_importance})\n",
    "\n",
    "# Sort the DataFrame of feature importance in descending order\n",
    "fi_df = fi_df.reindex(fi_df['t_stats'].abs().sort_values(ascending=False).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an Actual vs Predicted scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.7)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Actual vs Predicted: Linear Regression')\n",
    "plt.text(50, 300, f'R-squared: {r2:.2f}', fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "\n",
    "\n",
    "# Create a bar plot to visualize feature importances\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(fi_df['Feature'], fi_df['t_stats'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Feature Importance')\n",
    "plt.title('Feature Importance in Linear Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Compare the interpretation results for coefficient magnitude and feature importance methods. How do you describe the diffference between the two?  \n",
    "\n",
    "**Exercise 2:** Evaluate the satisfaction of homoscedasticity and multi-collinearity assumtions in this example. You can plot (use scatter plot) the error (y-axis) with respect to different features (x-axis) to evaluate the homoscedasticity assumption (the distribution of error aound zero should be normal). To check for multi-collinearity, you can plot heatmap of correlation between features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigatin the Fairness Issues using Pre-Processing: Removing Bais from Data\n",
    "\n",
    "We can use the `CorrelationRemover` function in `Fairlearn` package for this purpose. It removes sensitive features from data and regresses out the sensitive features from non-sensitive features and uses the residuals as corrected features for bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, roc_auc_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_ratio\n",
    "import seaborn as sns\n",
    "from fairlearn.preprocessing import CorrelationRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate synthetic features:\n",
    "# - Credit score (normally around 300-850)\n",
    "# - Annual income in thousands (normally around mean of $50k with std dev of $15k)\n",
    "# - Employment history in years (normally around 0-40)\n",
    "X = pd.DataFrame({\n",
    "    'Credit_Score': np.random.normal(650, 100, n),\n",
    "    'Annual_Income_k': np.random.normal(50, 15, n),\n",
    "    'Employment_History_y': np.random.normal(20, 10, n),\n",
    "    'Interest_Rate': np.random.normal(5, 1, n),\n",
    "    'Sex': np.random.choice([0, 1], n)  # 0 for 'Male', 1 for 'Female'\n",
    "})\n",
    "\n",
    "# Introduce a bias in the 'Interest_Rate' feature based on the sensitive attribute 'Sex'\n",
    "X.loc[X['Sex'] == 1, 'Interest_Rate'] += 2  # Females have higher interest rates on average\n",
    "\n",
    "# Introduce a strong bias in the label (loan approval) based on the sensitive attribute (sex)\n",
    "threshold_male = 1.5  # A relatively low bar for males\n",
    "threshold_female = 2.0  # A higher bar for females\n",
    "\n",
    "# Calculate a synthetic loan approval decision based on the financial features and bias\n",
    "\n",
    "y = pd.DataFrame({'Loan_Approval':np.zeros(X.shape[0])})\n",
    "y.loc[(X['Sex'] == 0) & (X['Credit_Score']/650 + X['Annual_Income_k']/50 > threshold_male), 'Loan_Approval'] = 1\n",
    "y.loc[(X['Sex'] == 1) & (X['Credit_Score']/650 + X['Annual_Income_k']/50 > threshold_female), 'Loan_Approval'] = 1\n",
    "\n",
    "selection_rates = MetricFrame(\n",
    "    metrics=selection_rate, y_true=y['Loan_Approval'], y_pred=y['Loan_Approval'], sensitive_features=X['Sex']\n",
    ")\n",
    "\n",
    "fig = selection_rates.by_group.plot.bar(\n",
    "    legend=False, rot=0, title=\"Fraction of Loan Approvals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the possible correlation between senstive and non-sensitive features using a heatmap plot of correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use the coorelartion remover to remove the bias from data. Mind that we fit it only on the training data and then transfom both training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "sensitive_test = X_test['Sex']\n",
    "\n",
    "cr = CorrelationRemover(sensitive_feature_ids=[\"Sex\"], alpha=0.5)\n",
    "X_train_cr = cr.fit_transform(X_train)\n",
    "X_train_cr = pd.DataFrame(X_train_cr, columns=['Credit_Score','Annual_Income_k','Employment_History_y','Interest_Rate'])\n",
    "X_test_cr = cr.transform(X_test)\n",
    "X_test_cr = pd.DataFrame(X_test_cr, columns=['Credit_Score','Annual_Income_k','Employment_History_y','Interest_Rate'])\n",
    "\n",
    "sns.heatmap(X_train_cr.corr(), annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_cr, y_train)\n",
    "y_pred = clf.predict(X_test_cr)\n",
    "y_pred_prob = clf.predict_proba(X_test_cr)[:,1]\n",
    "\n",
    "# Measure accuracy\n",
    "accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Measure AUC\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    count,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    "    selection_rate,\n",
    ")\n",
    "# Analyze metrics using MetricFrame\n",
    "metrics = {\n",
    "    \"accuracy\": balanced_accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "}\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test\n",
    ")\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    ylim=[0,1.05],\n",
    "    title=\"Show all metrics\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Statistical Parity\n",
    "stat_parity_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Statistical Parity Difference: {stat_parity_diff}\")\n",
    "\n",
    "stat_parity_rto = demographic_parity_ratio(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Statistical Parity ratio: {stat_parity_rto}\")\n",
    "\n",
    "# Equalized Odds\n",
    "equal_odds_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Equalized Odds Difference: {equal_odds_diff}\")\n",
    "\n",
    "equal_odds_rto = equalized_odds_ratio(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Equalized Odds Ratio: {equal_odds_rto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.3:** Try combining the preprocessing techniques, i.e, first resample the data and then apply the correlation remover. Report the results in comparison with two previous experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Interpretable Model-agnostic Explanations\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is another popular method for explaining machine learning models. It provides a unified measure of feature importance and can be used for both classification and regression models. Below is an example of how to use SHAP with a RandomForestClassifier for the binary classification problem.\n",
    "\n",
    "In this code:\n",
    "\n",
    "* We set a binary threshold on the median of y to create a binary classification target variable y_binary, where data points with y values above the median are labeled as 1 (high risk), and data points below or equal to the median are labeled as 0 (low risk).\n",
    "\n",
    "* We train a Gradient Boosting Classifier on the modified target variable y_binary.\n",
    "\n",
    "* We calculate the accuracy score on the test data to evaluate the classifier's performance.\n",
    "\n",
    "* We create a SHAP explainer using shap.Explainer and provide the model and training data.\n",
    "\n",
    "* We choose a prediction to explain (e.g., the first test data point), and we use explainer.shap_values to compute the SHAP values for that prediction.\n",
    "\n",
    "* We visualize the SHAP values using shap.summary_plot to see the impact of each feature on the prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the SHAP Python package\n",
    "!pip install shap\n",
    "# upgrading sklearn tot he latest version\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import shap\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes_data = load_diabetes(scaled=False)\n",
    "X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y = diabetes_data.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Create and fit the Random Forest Classifier model with two classes (0 and 1)\n",
    "# We are now using the original regression target variable y\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=random_state)\n",
    "model.fit(X_train, (y_train > np.median(y)).astype(int))  # Use binary target variable here\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score((y_test > np.median(y)).astype(int), y_pred)  # Use binary target variable here\n",
    "print(\"Accuracy on Test Data:\", accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The kernel explainer receives the function that predicts the probability of classes and the train set as its main inputs.\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "\n",
    "# Computing the SHAP values using the samples in the test set.\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# get just the explanations for the positive class\n",
    "shap_values = shap_values[..., 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a bar plot for plotting the mean absolute value of the SHAP values each feature to provide a global explanation of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use waterfall plot to illustrate the contribution of each feature in pushing the model output for a specific sample from the base value (the average model output over the training dataset we passed). Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. The plot has the following components:\n",
    "\n",
    "* Base Value (Expected Value): The plot starts with the base value, which represents the expected prediction of the model over the entire dataset (i.e., the average prediction if no specific feature information is available).\n",
    "This is usually shown as the first horizontal line on the plot.\n",
    "\n",
    "* Feature Contributions (Shapley Values): Each subsequent step in the waterfall plot corresponds to the contribution of a feature to the model's prediction for the specific instance being explained. Features are listed in order of their impact on the prediction, with the largest contributing features (positive or negative) typically shown first.\n",
    "\n",
    "* Positive and Negative Contributions: Features that increase the model’s prediction relative to the base value. They are represented by upward steps, moving the value higher. Features that decrease the prediction relative to the base value. These are shown as downward steps, lowering the prediction. Each step's length is proportional to the Shapley value of that feature, which represents how much that feature shifts the prediction from the base value.\n",
    "\n",
    "* Final Prediction Value: The final value after all contributions from the features have been added to (or subtracted from) the base value is the model’s prediction for that particular instance. This is usually shown at the far right of the plo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_id = 0\n",
    "shap.plots.waterfall(shap_values[sample_id])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3.3:** Try `shap.explain.Exact` and `shap.TreeExplainer` and compare the running time and final feature attribution with shap.KernelExplainer (global snd local for at least five patients)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

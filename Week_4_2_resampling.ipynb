{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigatin the Fairness Issues using Pre-Processing: Data Resampling Technique\n",
    "\n",
    "In data resampling, we try to ensure that all the sensitive groups are equally represented in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, precision_score, roc_auc_score\n",
    "from fairlearn.metrics import MetricFrame, selection_rate, demographic_parity_difference, equalized_odds_difference\n",
    "from fairlearn.metrics import demographic_parity_ratio, equalized_odds_ratio\n",
    "import seaborn as sns\n",
    "from fairlearn.preprocessing import CorrelationRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate synthetic features:\n",
    "# - Credit score (normally around 300-850)\n",
    "# - Annual income in thousands (normally around mean of $50k with std dev of $15k)\n",
    "# - Employment history in years (normally around 0-40)\n",
    "df = pd.DataFrame({\n",
    "    'Credit_Score': np.random.normal(650, 100, n),\n",
    "    'Annual_Income_k': np.random.normal(50, 15, n),\n",
    "    'Employment_History_y': np.random.normal(20, 10, n),\n",
    "    'Interest_Rate': np.random.normal(5, 1, n),\n",
    "    'Sex': np.random.choice([0, 1], n)  # 0 for 'Male', 1 for 'Female'\n",
    "})\n",
    "\n",
    "# Introduce a bias in the 'Interest_Rate' feature based on the sensitive attribute 'Sex'\n",
    "df.loc[df['Sex'] == 1, 'Interest_Rate'] += 2  # Females have higher interest rates on average\n",
    "\n",
    "# Introduce a strong bias in the label (loan approval) based on the sensitive attribute (sex)\n",
    "threshold_male = 1.5  # A relatively low bar for males\n",
    "threshold_female = 2.0  # A higher bar for females\n",
    "\n",
    "# Calculate a synthetic loan approval decision based on the financial features and bias\n",
    "df['Loan_Approval'] = 0\n",
    "df.loc[(df['Sex'] == 0) & (df['Credit_Score']/650 + df['Annual_Income_k']/50 > threshold_male), 'Loan_Approval'] = 1\n",
    "df.loc[(df['Sex'] == 1) & (df['Credit_Score']/650 + df['Annual_Income_k']/50 > threshold_female), 'Loan_Approval'] = 1\n",
    "\n",
    "selection_rates = MetricFrame(\n",
    "    metrics=selection_rate, y_true=df['Loan_Approval'], y_pred=df['Loan_Approval'], sensitive_features=df['Sex']\n",
    ")\n",
    "\n",
    "fig = selection_rates.by_group.plot.bar(\n",
    "    legend=False, rot=0, title=\"Fraction of Loan Approvals\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On training data, we can downsample the samples in each group based on the minimum number of samples in the minority group considering the labels. This ensures we have same selection rate across different sensitive groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "group_0 = train[train['Sex'] == 0]\n",
    "group_1 = train[train['Sex'] == 1]\n",
    "\n",
    "# Function to resample each subgroup to have the same number of samples\n",
    "def resample_group(df, group_size):\n",
    "    return df.sample(n=group_size, replace=True, random_state=42)\n",
    "\n",
    "# Calculate the smallest number of samples among the groups\n",
    "min_group_size = min(len(group_0[group_0['Loan_Approval'] == 1]), len(group_1[group_1['Loan_Approval'] == 1]))\n",
    "\n",
    "# Resample each group to have the same number of samples\n",
    "group_0_approved = resample_group(group_0[group_0['Loan_Approval'] == 1], min_group_size)\n",
    "group_1_approved = resample_group(group_1[group_1['Loan_Approval'] == 1], min_group_size)\n",
    "\n",
    "group_0_rejected = resample_group(group_0[group_0['Loan_Approval'] == 0], min_group_size)\n",
    "group_1_rejected = resample_group(group_1[group_1['Loan_Approval'] == 0], min_group_size)\n",
    "\n",
    "# Combine the resampled data back\n",
    "train_resampled = pd.concat([group_0_approved, group_0_rejected, group_1_approved, group_1_rejected])\n",
    "\n",
    "# Shuffle the resampled data\n",
    "train_resampled = train_resampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Prepare data for model training\n",
    "X_train_resampled = train_resampled.drop(columns=['Loan_Approval', 'Sex'])\n",
    "y_train_resampled = train_resampled['Loan_Approval']\n",
    "\n",
    "X_test = test.drop(columns=['Loan_Approval','Sex'])\n",
    "y_test = test['Loan_Approval']\n",
    "sensitive_test = test['Sex']\n",
    "\n",
    "selection_rates = MetricFrame(\n",
    "    metrics=selection_rate, y_true=train_resampled['Loan_Approval'], y_pred=train_resampled['Loan_Approval'], sensitive_features=train_resampled['Sex']\n",
    ")\n",
    "\n",
    "fig = selection_rates.by_group.plot.bar(\n",
    "    legend=False, rot=0, title=\"Fraction of Loan Approvals\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train_resampled, y_train_resampled)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Measure accuracy\n",
    "accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "# Measure AUC\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "print(f\"AUC: {auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.metrics import (\n",
    "    MetricFrame,\n",
    "    count,\n",
    "    false_negative_rate,\n",
    "    false_positive_rate,\n",
    "    selection_rate,\n",
    ")\n",
    "# Analyze metrics using MetricFrame\n",
    "metrics = {\n",
    "    \"accuracy\": balanced_accuracy_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"false positive rate\": false_positive_rate,\n",
    "    \"false negative rate\": false_negative_rate,\n",
    "    \"selection rate\": selection_rate,\n",
    "}\n",
    "metric_frame = MetricFrame(\n",
    "    metrics=metrics, y_true=y_test, y_pred=y_pred, sensitive_features=sensitive_test\n",
    ")\n",
    "metric_frame.by_group.plot.bar(\n",
    "    subplots=True,\n",
    "    layout=[3, 3],\n",
    "    legend=False,\n",
    "    figsize=[12, 8],\n",
    "    ylim=[0,1.05],\n",
    "    title=\"Show all metrics\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Statistical Parity\n",
    "stat_parity_diff = demographic_parity_difference(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Statistical Parity Difference: {stat_parity_diff}\")\n",
    "\n",
    "stat_parity_rto = demographic_parity_ratio(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Statistical Parity ratio: {stat_parity_rto}\")\n",
    "\n",
    "# Equalized Odds\n",
    "equal_odds_diff = equalized_odds_difference(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Equalized Odds Difference: {equal_odds_diff}\")\n",
    "\n",
    "equal_odds_rto = equalized_odds_ratio(y_test, y_pred, sensitive_features=sensitive_test)\n",
    "print(f\"Equalized Odds Ratio: {equal_odds_rto}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4.2:** Try implementing an upsampling method (rather than downsampling), and evalute its effect on the accuracy and fairness of the model. Discuss when we should prefer one approach over the other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

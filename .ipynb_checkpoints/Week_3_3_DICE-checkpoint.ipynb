{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DICE: Diverse Counterfactual Explanation\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is another popular method for explaining machine learning models. It provides a unified measure of feature importance and can be used for both classification and regression models. Below is an example of how to use SHAP with a RandomForestClassifier for the binary classification problem.\n",
    "\n",
    "In this code:\n",
    "\n",
    "* We set a binary threshold on the median of y to create a binary classification target variable y_binary, where data points with y values above the median are labeled as 1 (high risk), and data points below or equal to the median are labeled as 0 (low risk).\n",
    "\n",
    "* We train a Gradient Boosting CLassifier on the modified target variable y_binary.\n",
    "\n",
    "* We calculate the accuracy score on the test data to evaluate the classifier's performance.\n",
    "\n",
    "* We create a DICE explainer using dice_ml.Dice. We specify the model, backend, data, data type, and target name.\n",
    "\n",
    "* We use DICE's generate_counterfactuals function to generate counterfactual explanations for the chosen prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dice-ml\n",
      "  Downloading dice_ml-0.11-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\diego\\anaconda3\\lib\\site-packages (from dice-ml) (4.19.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\diego\\anaconda3\\lib\\site-packages (from dice-ml) (1.22.4)\n",
      "Requirement already satisfied: pandas<2.0.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from dice-ml) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\diego\\anaconda3\\lib\\site-packages (from dice-ml) (1.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\diego\\anaconda3\\lib\\site-packages (from dice-ml) (4.66.5)\n",
      "Collecting raiutils>=0.4.0 (from dice-ml)\n",
      "  Downloading raiutils-0.4.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from pandas<2.0.0->dice-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from pandas<2.0.0->dice-ml) (2024.1)\n",
      "Requirement already satisfied: requests in c:\\users\\diego\\anaconda3\\lib\\site-packages (from raiutils>=0.4.0->dice-ml) (2.32.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\diego\\anaconda3\\lib\\site-packages (from raiutils>=0.4.0->dice-ml) (1.10.1)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (6.4.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (2023.7.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from jsonschema->dice-ml) (0.10.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn->dice-ml) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn->dice-ml) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\diego\\anaconda3\\lib\\site-packages (from tqdm->dice-ml) (0.4.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from importlib-resources>=1.4.0->jsonschema->dice-ml) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas<2.0.0->dice-ml) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from requests->raiutils>=0.4.0->dice-ml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from requests->raiutils>=0.4.0->dice-ml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from requests->raiutils>=0.4.0->dice-ml) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from requests->raiutils>=0.4.0->dice-ml) (2024.8.30)\n",
      "Downloading dice_ml-0.11-py3-none-any.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 20.6 MB/s eta 0:00:00\n",
      "Downloading raiutils-0.4.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: raiutils, dice-ml\n",
      "Successfully installed dice-ml-0.11 raiutils-0.4.2\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\diego\\anaconda3\\lib\\site-packages (1.3.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\diego\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Downloading scikit_learn-1.3.2-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "   ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 4.2/9.3 MB 22.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.3/9.3 MB 23.0 MB/s eta 0:00:00\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "Successfully installed scikit-learn-1.3.2\n"
     ]
    }
   ],
   "source": [
    "# Installing the DICE Python package\n",
    "!pip install dice-ml\n",
    "# upgrading sklearn tot he latest version\n",
    "!pip install scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Data: 0.7191011235955056\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import dice_ml\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes_data = load_diabetes(scaled=False)\n",
    "X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y = diabetes_data.target\n",
    "\n",
    "# Set a threshold for binary classification (e.g., using the median of y)\n",
    "threshold = np.median(y)\n",
    "y_binary = (y > threshold).astype(int)  # 1 for high risk, 0 for low risk\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=random_state)\n",
    "\n",
    "# Create and fit the Random Forest Classifier model\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)  # Use binary target variable here\n",
    "print(\"Accuracy on Test Data:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DICE data interface\n",
    "d = dice_ml.Data(dataframe=pd.concat([X_test,pd.DataFrame({'target':y_test})], axis=1), \n",
    "                 features={\n",
    "                   'age':[1, 130],\n",
    "                   'bmi': [10,50],\n",
    "                   'bp': [50,150],\n",
    "                   },\n",
    "                 continuous_features=['age', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'], \n",
    "                 outcome_name='target')\n",
    "\n",
    "# Create a DICE model interface\n",
    "m = dice_ml.Model(model=model, backend=\"sklearn\")\n",
    "\n",
    "# Create a DICE explainer\n",
    "exp = dice_ml.Dice(d, m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  6.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query instance (original outcome : 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.400002</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   bmi    bp     s1         s2    s3   s4      s5    s6  target\n",
       "0  29.0  1.0  30.0  85.0  180.0  93.400002  43.0  4.0  5.3845  88.0       1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Diverse Counterfactual set (new outcome: 0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>70.4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>106.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.1</td>\n",
       "      <td>85.0</td>\n",
       "      <td>178.4</td>\n",
       "      <td>93.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.7</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>84.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>56.7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>101.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>68.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>109.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.6</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.6089</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>34.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.2</td>\n",
       "      <td>85.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>93.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.3845</td>\n",
       "      <td>88.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  sex   bmi    bp     s1    s2    s3   s4      s5     s6  target\n",
       "0  29.0  1.0  30.0  85.0  180.0  93.4  70.4  4.0  5.3845  106.7       0\n",
       "1  29.0  1.0  20.4  85.0  180.0  93.4  43.0  7.1  5.3845   88.0       0\n",
       "2  29.0  1.0  19.1  85.0  178.4  93.4  43.0  4.0  5.3845   88.0       0\n",
       "3  29.0  1.0  31.7  85.0  180.0  93.4  84.8  4.0  5.3845   88.0       0\n",
       "4  29.0  1.0  30.0  85.0  180.0  93.4  56.7  4.0  5.3845  101.4       0\n",
       "5  29.0  1.0  30.0  85.0  180.0  93.4  68.0  4.0  5.3845  109.3       0\n",
       "6  29.0  1.0  20.0  85.0  180.0  93.4  43.0  4.1  5.3845   88.0       0\n",
       "7  29.0  1.0  21.6  85.0  180.0  93.4  43.0  4.0  3.6089   88.0       0\n",
       "8  29.0  1.0  18.4  85.0  180.0  93.4  34.1  4.0  5.3845   88.0       0\n",
       "9  29.0  1.0  18.2  85.0  180.0  93.4  43.0  4.0  5.3845   88.0       0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose a prediction to explain (e.g., the first test data point)\n",
    "\n",
    "sample_id = 20\n",
    "\n",
    "query_instance = X_test.iloc[sample_id:sample_id+1,:]\n",
    "\n",
    "# Generate 10 counterfactual examples \n",
    "# We can decide which feature to vary so the rest remain immutable (in this case Age and Sex)\n",
    "counterfactuals = exp.generate_counterfactuals(query_instance, total_CFs=10, desired_class='opposite',\n",
    "                                               features_to_vary=['bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6'],\n",
    "                                                proximity_weight=0.5, diversity_weight=1.0,)\n",
    "\n",
    "# List the counterfactual examples\n",
    "counterfactuals.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise3.4:** First learn more about `proximity_weight` (default is 0.5) and `diversity_weight` (default is 1), the input arguments to `generate_counterfactuals`, and then play with them to manipulate the proximity and diversity of the generated samples. Discuss how they may affect the explanation of the model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(exp) # running the prior reveals that exp is a dice_random object "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the documentation of the dice_random module the parameters proximity weight and diversity weight are not used the explainer object created is an instance of the dice_random module. Which generates counterfactuals using an algorithm specific to this module. \n",
    "\n",
    "However, these parameters are used in other dice modules such as dice_genetic and both dice_tensorflow1 and dice_tensorflow2 which are also used to generate counterfactuals. \n",
    "\n",
    "In the cases where these parameters are used the objective function being minimized computes loss with respect to desired counterfactual prediction and computed counterfactual prediction, proximity of the feature values of counterfactuals to the observed instance, and the proximity of the counterfactuals' feature values to each other (this being the diversity loss). The proximity loss aims to penalize counterfactuals that are increasingly different to the observation, and the diversity loss aims to penalize combinations of counterfactuals that are similar to each other. The proximity_weight and diversity_weight hyperparameters regulate the strength of their respective loss functions. The more you increase each one the more likely it is to fulfill the proximity loss or diversity loss criteria. Tuning these parameters at the same time can help the user find a balance between distance to the original observation and diversity of explanations for counterfactual outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DiceRandom in module dice_ml.explainer_interfaces.dice_random object:\n",
      "\n",
      "class DiceRandom(dice_ml.explainer_interfaces.explainer_base.ExplainerBase)\n",
      " |  DiceRandom(data_interface, model_interface)\n",
      " |  \n",
      " |  Helper class that provides a standard way to create an ABC using\n",
      " |  inheritance.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DiceRandom\n",
      " |      dice_ml.explainer_interfaces.explainer_base.ExplainerBase\n",
      " |      abc.ABC\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, data_interface, model_interface)\n",
      " |      Init method\n",
      " |      \n",
      " |      :param data_interface: an interface class to access data related params.\n",
      " |      :param model_interface: an interface class to access trained ML model.\n",
      " |  \n",
      " |  get_continuous_samples(self, low, high, precision, size=1000, seed=None)\n",
      " |  \n",
      " |  get_samples(self, fixed_features_values, feature_range, sampling_random_seed, sampling_size)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from dice_ml.explainer_interfaces.explainer_base.ExplainerBase:\n",
      " |  \n",
      " |  build_KD_tree(self, data_df_copy, desired_range, desired_class, predicted_outcome_name)\n",
      " |  \n",
      " |  check_permitted_range(self, permitted_range)\n",
      " |      checks permitted range for continuous features\n",
      " |      TODO: add comments as to where this is used if this function is necessary, else remove.\n",
      " |  \n",
      " |  check_query_instance_validity(self, features_to_vary, permitted_range, query_instance, feature_ranges_orig)\n",
      " |  \n",
      " |  decide_cf_validity(self, model_outputs)\n",
      " |  \n",
      " |  decode_model_output(self, encoded_labels)\n",
      " |  \n",
      " |  decode_to_original_labels(self, test_instance_df, final_cfs_df, final_cfs_df_sparse)\n",
      " |  \n",
      " |  do_binary_search(self, diff, decimal_prec, query_instance, cf_ix, feature, final_cfs_sparse, current_pred)\n",
      " |      Performs a binary search between continuous features of a CF and corresponding values\n",
      " |      in query_instance until the prediction class changes.\n",
      " |  \n",
      " |  do_linear_search(self, diff, decimal_prec, query_instance, cf_ix, feature, final_cfs_sparse, current_pred_orig, limit_steps_ls)\n",
      " |      Performs a greedy linear search - moves the continuous features in CFs towards original values in\n",
      " |      query_instance greedily until the prediction class changes, or it reaches the maximum number of steps\n",
      " |  \n",
      " |  do_posthoc_sparsity_enhancement(self, final_cfs_sparse, query_instance, posthoc_sparsity_param, posthoc_sparsity_algorithm, limit_steps_ls)\n",
      " |      Post-hoc method to encourage sparsity in a generated counterfactuals.\n",
      " |      \n",
      " |      :param final_cfs_sparse: Final CFs in original user-fed format, in a pandas dataframe.\n",
      " |      :param query_instance: Query instance in original user-fed format, in a pandas dataframe.\n",
      " |      :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity.\n",
      " |      :param posthoc_sparsity_algorithm: Perform either linear or binary search.\n",
      " |                                         Prefer binary search when a feature range is\n",
      " |                                         large (for instance, income varying from 10k to 1000k)\n",
      " |                                         and only if the features share a monotonic relationship\n",
      " |                                         with predicted outcome in the model.\n",
      " |      :param limit_steps_ls: Defines the limit of steps to be done in the linear search,\n",
      " |                              necessary to avoid infinite loops\n",
      " |  \n",
      " |  feature_importance(self, query_instances, cf_examples_list=None, total_CFs=10, local_importance=True, global_importance=True, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, posthoc_sparsity_algorithm='linear', **kwargs)\n",
      " |      Estimate feature importance scores for the given inputs.\n",
      " |      \n",
      " |      :param query_instances: A list of inputs for which to compute the\n",
      " |                              feature importances. These can be provided as a dataframe.\n",
      " |      :param cf_examples_list: If precomputed, a list of counterfactual\n",
      " |                               examples for every input point. If cf_examples_list is provided, then\n",
      " |                               all the following parameters are ignored.\n",
      " |      :param total_CFs: The number of counterfactuals to generate per input\n",
      " |                        (default is 10)\n",
      " |      :param other_parameters: These are the same as the generate_counterfactuals method.\n",
      " |      \n",
      " |      :returns: An object of class CounterfactualExplanations that includes\n",
      " |                the list of counterfactuals per input, local feature importances per\n",
      " |                input, and the global feature importance summarized over all inputs.\n",
      " |  \n",
      " |  generate_counterfactuals(self, query_instances, total_CFs, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, proximity_weight=0.2, sparsity_weight=0.2, diversity_weight=5.0, categorical_penalty=0.1, posthoc_sparsity_algorithm='linear', verbose=False, **kwargs)\n",
      " |      General method for generating counterfactuals.\n",
      " |      \n",
      " |      :param query_instances: Input point(s) for which counterfactuals are to be generated.\n",
      " |                              This can be a dataframe with one or more rows.\n",
      " |      :param total_CFs: Total number of counterfactuals required.\n",
      " |      :param desired_class: Desired counterfactual class - can take 0 or 1. Default value\n",
      " |                            is \"opposite\" to the outcome class of query_instance for binary classification.\n",
      " |      :param desired_range: For regression problems. Contains the outcome range to\n",
      " |                            generate counterfactuals in. This should be a list of two numbers in\n",
      " |                            ascending order.\n",
      " |      :param permitted_range: Dictionary with feature names as keys and permitted range in list as values.\n",
      " |                              Defaults to the range inferred from training data.\n",
      " |                              If None, uses the parameters initialized in data_interface.\n",
      " |      :param features_to_vary: Either a string \"all\" or a list of feature names to vary.\n",
      " |      :param stopping_threshold: Minimum threshold for counterfactuals target class probability.\n",
      " |      :param proximity_weight: A positive float. Larger this weight, more close the counterfactuals are to the\n",
      " |                               query_instance. Used by ['genetic', 'gradientdescent'],\n",
      " |                               ignored by ['random', 'kdtree'] methods.\n",
      " |      :param sparsity_weight: A positive float. Larger this weight, less features are changed from the query_instance.\n",
      " |                              Used by ['genetic', 'kdtree'], ignored by ['random', 'gradientdescent'] methods.\n",
      " |      :param diversity_weight: A positive float. Larger this weight, more diverse the counterfactuals are.\n",
      " |                               Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n",
      " |      :param categorical_penalty: A positive float. A weight to ensure that all levels of a categorical variable sums to 1.\n",
      " |                               Used by ['genetic', 'gradientdescent'], ignored by ['random', 'kdtree'] methods.\n",
      " |      :param posthoc_sparsity_param: Parameter for the post-hoc operation on continuous features to enhance sparsity.\n",
      " |      :param posthoc_sparsity_algorithm: Perform either linear or binary search. Takes \"linear\" or \"binary\".\n",
      " |                                         Prefer binary search when a feature range is large (for instance,\n",
      " |                                         income varying from 10k to 1000k) and only if the features share a\n",
      " |                                         monotonic relationship with predicted outcome in the model.\n",
      " |      :param verbose: Whether to output detailed messages.\n",
      " |      :param sample_size: Sampling size\n",
      " |      :param random_seed: Random seed for reproducibility\n",
      " |      :param kwargs: Other parameters accepted by specific explanation method\n",
      " |      \n",
      " |      :returns: A CounterfactualExplanations object that contains the list of\n",
      " |                counterfactual examples per query_instance as one of its attributes.\n",
      " |  \n",
      " |  get_model_output_from_scores(self, model_scores)\n",
      " |  \n",
      " |  global_feature_importance(self, query_instances, cf_examples_list=None, total_CFs=10, local_importance=True, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, posthoc_sparsity_algorithm='linear', **kwargs)\n",
      " |      Estimate global feature importance scores for the given inputs.\n",
      " |      \n",
      " |      :param query_instances: A list of inputs for which to compute the\n",
      " |                              feature importances. These can be provided as a dataframe.\n",
      " |      :param cf_examples_list: If precomputed, a list of counterfactual\n",
      " |                               examples for every input point. If cf_examples_list is provided, then\n",
      " |                               all the following parameters are ignored.\n",
      " |      :param total_CFs: The number of counterfactuals to generate per input\n",
      " |                        (default is 10)\n",
      " |      :param local_importance: Binary flag indicating whether local feature\n",
      " |                               importance values should also be returned for each query instance.\n",
      " |      :param other_parameters: These are the same as the generate_counterfactuals method.\n",
      " |      \n",
      " |      :returns: An object of class CounterfactualExplanations that includes\n",
      " |                the list of counterfactuals per input, local feature importances per\n",
      " |                input, and the global feature importance summarized over all inputs.\n",
      " |  \n",
      " |  infer_target_cfs_class(self, desired_class_input, original_pred, num_output_nodes)\n",
      " |      Infer the target class for generating CFs. Only called when\n",
      " |      model_type==\"classifier\".\n",
      " |      TODO: Add support for opposite desired class in multiclass.\n",
      " |      Downstream methods should decide whether it is allowed or not.\n",
      " |  \n",
      " |  infer_target_cfs_range(self, desired_range_input)\n",
      " |  \n",
      " |  is_cf_valid(self, model_score)\n",
      " |      Check if a cf belongs to the target class or target range.\n",
      " |  \n",
      " |  local_feature_importance(self, query_instances, cf_examples_list=None, total_CFs=10, desired_class='opposite', desired_range=None, permitted_range=None, features_to_vary='all', stopping_threshold=0.5, posthoc_sparsity_param=0.1, posthoc_sparsity_algorithm='linear', **kwargs)\n",
      " |      Estimate local feature importance scores for the given inputs.\n",
      " |      \n",
      " |      :param query_instances: A list of inputs for which to compute the\n",
      " |                              feature importances. These can be provided as a dataframe.\n",
      " |      :param cf_examples_list: If precomputed, a list of counterfactual\n",
      " |                               examples for every input point. If cf_examples_list is provided, then\n",
      " |                               all the following parameters are ignored.\n",
      " |      :param total_CFs: The number of counterfactuals to generate per input\n",
      " |                        (default is 10)\n",
      " |      :param other_parameters: These are the same as the\n",
      " |                               generate_counterfactuals method.\n",
      " |      \n",
      " |      :returns: An object of class CounterfactualExplanations that includes\n",
      " |                the list of counterfactuals per input, local feature importances per\n",
      " |                input, and the global feature importance summarized over all inputs.\n",
      " |  \n",
      " |  misc_init(self, stopping_threshold, desired_class, desired_range, test_pred)\n",
      " |  \n",
      " |  predict_fn(self, input_instance)\n",
      " |      prediction function\n",
      " |  \n",
      " |  predict_fn_for_sparsity(self, input_instance)\n",
      " |      prediction function for sparsity correction\n",
      " |  \n",
      " |  round_to_precision(self)\n",
      " |  \n",
      " |  serialize_explainer(self, path)\n",
      " |      Serialize the explainer to the file specified by path.\n",
      " |  \n",
      " |  setup(self, features_to_vary, permitted_range, query_instance, feature_weights)\n",
      " |  \n",
      " |  sigmoid(self, z)\n",
      " |      This is used in VAE-based CF explainers.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from dice_ml.explainer_interfaces.explainer_base.ExplainerBase:\n",
      " |  \n",
      " |  deserialize_explainer(path)\n",
      " |      Reload the explainer into the memory by reading the file specified by path.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from dice_ml.explainer_interfaces.explainer_base.ExplainerBase:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dice_ml.Dice(d, m) has as default the method \"random\", however the parameters proximity_weight and diversity_weight are not found in the code \n",
    "# of a dice_random object. However they are present in the dice_genetic object, meaning that it is this algorithm that is generating our counterfactuals.\n",
    "\n",
    "# from help(exp.generate_counterfactuals)  \n",
    "# from help(dice_ml.)  \n",
    "\n",
    "# dice_ml.Dice algorithm creates an interface between model, data, and has as deafault the method \"random\" for generating counterfactuals.\n",
    "# proximity_weight // A positive float. Larger this weight, more close the counterfactuals are to the query_instance. \n",
    "    # calls for the computation of proximity_loss function \n",
    "# diversity_weight // A positive float. Larger this weight, more diverse the counterfactuals are.   \n",
    "help(exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the \"Diabetes\" dataset again and apply the feature importance approach using Decision Trees to interpret the importance of features. We'll use scikit-learn to build a Decision Tree model and extract the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# Load the Diabetes dataset\n",
    "diabetes_data = load_diabetes(scaled=False)\n",
    "X = pd.DataFrame(diabetes_data.data, columns=diabetes_data.feature_names)\n",
    "y = diabetes_data.target \n",
    "\n",
    "# binarizing the target\n",
    "y = y>=np.median(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit the Random Forest Regressor model\n",
    "model = DecisionTreeClassifier(random_state=random_state)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Classifier Accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance in Decision Tree\n",
    "\n",
    "In a binary Decision Tree, if we represent the set of all nodes with $A$ and the set of all nodes splitting on the $i$ th feature with $B_i$, then the feature importance $F_i$ for the $i$ th feature is computed as follows:\n",
    "\n",
    "$F_i = \\frac{\\sum_{j \\in B_i} N_j}{\\sum_{j \\in A} N_j}$\n",
    "\n",
    "Here, $N_j$ represents the node importance for the $j$ th node that is computed using the following formula:\n",
    "\n",
    "$N_j = p_j \\times  M_j - p_j^{Left} \\times M_j^{Left} -  p_j^{Right} \\times M_j^{Right}$\n",
    "\n",
    "where $p_j$, $p_j^{Left}$, and $p_j^{Right}$ represent respectively the percentage of samples reaching the $j$ th node, the left subtree and the right subtree of the $j$ the node. Similarly, $M_j$, $M_j^{Left}$, and $M_j^{Right}$ represent respectively the node impurity for the $j$ th node, the left subtree and the right subtree of the $j$ the node.\n",
    "\n",
    "Depnding on the task (classification or regression) and the splitting criterion, the $M_j$, the node impurity for the $j$ th node, is computed as follows:\n",
    "\n",
    "|   Task         |   Impurity  |   Formula  | Description  |\n",
    "| :---:          |    :----:   |   :----:   |          :---: |\n",
    "| Classification | Gini        |$1 - \\sum_{k=1}^{C} f_k^2$              | $f_k$ is the frequency of the label $k$ at a node and C is the number of target classes.|\n",
    "| Classification | Entropy     |$\\sum_{k=1}^{C} -f_k \\log(f_k)$          | $f_k$ is the frequency of the label $k$ at a node and C is the number of target classes.|\n",
    "| Regression     | MSE         |$\\frac{1}{N}\\sum_{k=1}^{N} (y_k-\\mu)^2$  | $N$ is the number of samples, $y_k$ is the value for the $k$ th target, and $\\mu$ is average of $y_k$ s.|\n",
    "| Regression     | MAE         |$\\frac{1}{N}\\sum_{k=1}^{N} abs(y_k-\\mu)$ | $N$ is the number of samples, $y_k$ is the value for the $k$ th target, and $\\mu$ is average of $y_k$ s.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the feature importances and feature names\n",
    "importances = model.feature_importances_\n",
    "feature_names = diabetes_data.feature_names\n",
    "\n",
    "# Create a DataFrame to store the importances with their corresponding feature names\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the DataFrame by importance in descending order\n",
    "importances_df = importances_df.reindex(importances_df['Importance'].abs().sort_values(ascending=False).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an Actual vs Predicted scatter plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.bar(importances_df['Feature'], importances_df['Importance'])\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance: Decision Tree')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot will display the feature importances calculated by the Decision Tree model for the \"Diabetes\" dataset. Larger bars indicate higher feature importance, and the sorted order of features helps us understand which features contribute the most to the model's predictions based on the reduction in impurity.\n",
    "\n",
    "In Decision Trees, the feature importances are calculated based on how much each feature contributes to reducing the impurity (e.g., Gini impurity or entropy) when splitting the data at each node. Features with higher importances have more significant influence on the tree's decisions, as they result in more homogeneous child nodes after splitting.\n",
    "\n",
    "**Exercise 4:** Try different random states when defining decision tree model. How does it affect the final feature importance results?\n",
    "\n",
    "**Exercise 5:** How can we explain the difference between the feature importance in logistic regression and decision tree?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
